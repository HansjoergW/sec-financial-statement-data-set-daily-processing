Next
====



- Aufbau erweiterbarer "PRE" Test, in welchem mann spezialfälle direkt in ein testdaten verzeichnis ablegen
  kann. der Test iteriert dann über alle Testfiles und prüft die Ergebnisse.
  Die Prüfungen müsste man vermutlich auch irgendwie standardisieren, bzw. die erwartete CSV auch Datei hinstellen.


- berechnen pre-xml
 - vlt sollte man sich zuerst nur auf fehlende reports konzentrieren
   -> könnte man höchstens über die anzahl lösen ->
   -> das grösste problem ist aber, dass die report nummer nicht stimmt
   -> vlt wäre es besser, zuerst nur stmt:anzahl Zeilen zu vergleichen, also wirklich eine spezielles report tag  zu generieren
 - nicht beachten der speziellen "srt" Zeilen


- quartal vergleichen
-- pre zip
    # problem, die gruppierung muss beim vergleich beachtet werden.
    # es wird so eine art universelle gruppe benötigt.. so was wie
    # statement-anz rows..
    # vlt. würde es auch helfen, wenn nur die relevanten statements gefiltert
    # werden. Vlt würde dann die Nummerierung stimmen..


- status index file über https://www.sec.gov/Archives/edgar/monthly/index.json auswerten
  -> File SecIndexIndexJsonParsing.py
  -> man sieht die letzte Bearbeitung
  -> man sieht die letzte Datei


- bessere und klarere Log Statements.. irgendwie standardisieren. Vlt auch so etwas wie eine processstatus am Ende

Later
=====

Ziel: laufendes herunterladen der neuen Reports als XML und bereitstellen der Daten als CSV Datei

- Massentest über ganzes Quartal
-- XML-Dateien für komplettes Quartal downloaden
-- Vergleich mit txt-Daten aus diesem Quartal
--- Alle Reports (10-K/Q) müssen vorhanden sein
--- Die txt-Dateien dürfen keine Zeilen enthalten, die nicht in den XML Daten vorhanden sind
--- Prüfen ob

- zippen von xml dateien

- Reihenfolge der Spalten in den geparsten CSVs in der selben Reihenfolge wie original, damit es änlicher iwrd

-handhaben von
   3.4. there are some rare cases of statements (4 statements in 5500 reports for q1 2021) that contain multiple root_nodes.
     -> not supported yet. -> idee, zählen der Kinder und den Root mit den meisten children nehmen
     0000829224-21-000029 http://www.starbucks.com/role/DocumentAndEntityInformation
     0000920371-21-000042 http://www.simpsonfg.com/role/ConsolidatedStatementsofStockholdersEquityParenthetical
     0001254699-21-000005 http://www.qvc.com/role/ConsolidatedStatementsofCashFlows
     0001628280-21-002278 http://polaris.com/role/ConsolidatedStatementsOfCashFlows

    BalanceSheet ist als CashFlow angegeben
     0001213900-21-019311 BS: sieht aus als wäre das als CashFlow betitelt, entries deuten aber auf BS hin


Done
====
21.05.2021 - erfolgreiches parsen aller reports in q1 2021 ohne überraschungen..
14.05.2021 - diverse optimierungen in pre-parsing
           - neu download mit prüfung auf grösse, da irgendwie die falschen Daten vorhanden sind...
           - donwload-xml Namen mit ADSH, damit eindeutigkeit sicher gestellt ist
13.05.2021 - diverse pre-parsing korrekturen
11.05.2021 - organistaion directory
               -https://docs.python-guide.org/writing/structure/
09.05.2021  - Masstest für teiltestset mit erneutem parsing
            - nur relevante statements parsen
08.05.2021  - line Eintrag aufgrund Hierarchie berechnet
04.05.2021  - alle Daten nochmals durchparsen
03.05.2021  - Fehler prüfen -> "huge text node"
02.05.2021  - Quartal durchparsen -> nur 2 'leere' numfiles können nicht vearbeitet werden
            - Automatisierung des Downloads und aufbereiten der Daten
            -- Die Daten werden laufend nachgeladen und aufbereitet
            -- Pro Report werden die Daten in eigene Dateien in Jahr/Monats/Tag (processingday)
30.04.2021  - pre und num files erzeugen..
            -- erste Version läuft, nächster Schritt komplettes quartal durchparsen
            - IFRS Standard Beispiel finden und prüfen
26.04.2021  - aufteilen code für index und xml files
22.04.2021  - Verwaltung Index Files
             -- bereits abgeschlossene nicht nochmals neuladen -> Status in Progress / finished
             -- finished wird erreicht wenn neues File vorhanden ist
09.04.2021  - Automatisierung des Downloads und aufbereiten der Daten
            -- in die DB soll auch der Name des SecFeedFiles aufgenommen werden, aus welchem die Daten stammen
            -- vor dem Inserten der Daten müssen die Einträge entfernt werden, die bereits vorhanden sind
            --- Spalte mit SecFeedFilenamen füllen
            --- Daten mit Schlüssel SecFeedFile laden und aufgrund adsh bereinigen
09.04.2021  - Sicherstellen, dass das Ergänzen der Informationen funktioniert.
              kleinere Pakete sichern, nicht alles auf einmal
09.04.2021  - Duplicate Check hinzufügen -> sicherstellen, dass jede ADSH nur einmal vorhanden ist.
              Offenbar gibt es mit den März Daten Probleme, entweder doppelt in der Datei, oder aber bereits im Februar vorhanden
05.04.2021  Flywayconfig  und installation