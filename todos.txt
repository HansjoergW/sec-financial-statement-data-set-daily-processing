Next
====

- Die Namen der runtergeladenen Dateien sind nicht eindeutig. Es muss daher immer noch die adsh_nummer beim Nahmen hinzugefügt werden.
  !!!

- Pre Spezialfälle
    - es scheint, als würden files manchmal nicht komplett runtergeladen. Fall tratt bei 0000004904-21-000010 auf.
      Das heruntergeladene xml war nicht komplett, obwohl das xml richtig abgeschlossen war.
      ein einzelner Download der Datei hat geklappt.
    - "nummerierte Tags", die dann keine to from beziehung mehr ermöglichen:
       0000018255-21-000004, 0000016160-21-000018

    - Klassifikation von IS in zip ist falsch, müsste CI sein: '0000016918-21-000010', 0000024090-21-000012" "0000034903-21-000020"
      - im Q1 2021 gibt es nur einen report, bei dem CI vorhanden ist, aber IS nicht (0001628280-21-003313).
        demgegenüber stehen 3473 Reports, bei denen ein IS vorhanden ist, aber kein CI
        so gesehen wäre das vorgehen, falls kein IS vorhanden ist CI zu verwenden ok



- Aufbau erweiterbarer "PRE" Test, in welchem mann spezialfälle direkt in ein testdaten verzeichnis ablegen
  kann. der Test iteriert dann über alle Testfiles und prüft die Ergebnisse.
  Die Prüfungen müsste man vermutlich auch irgendwie standardisieren, bzw. die erwartete CSV auch Datei hinstellen.



- berechnen pre-xml
 - vlt sollte man sich zuerst nur auf fehlende reports konzentrieren
   -> könnte man höchstens über die anzahl lösen ->
   -> das grösste problem ist aber, dass die report nummer nicht stimmt
   -> vlt wäre es besser, zuerst nur stmt:anzahl Zeilen zu vergleichen, also wirklich eine spezielles report tag  zu generieren
 - nicht beachten der speziellen "srt" Zeilen


- quartal vergleichen
-- pre zip
    # problem, die gruppierung muss beim vergleich beachtet werden.
    # es wird so eine art universelle gruppe benötigt.. so was wie
    # statement-anz rows..
    # vlt. würde es auch helfen, wenn nur die relevanten statements gefiltert
    # werden. Vlt würde dann die Nummerierung stimmen..


- status index file über https://www.sec.gov/Archives/edgar/monthly/index.json auswerten
  -> File SecIndexIndexJsonParsing.py
  -> man sieht die letzte Bearbeitung
  -> man sieht die letzte Datei


- bessere und klarere Log Statements.. irgendwie standardisieren. Vlt auch so etwas wie eine processstatus am Ende

Later
=====

Ziel: laufendes herunterladen der neuen Reports als XML und bereitstellen der Daten als CSV Datei

- Massentest über ganzes Quartal
-- XML-Dateien für komplettes Quartal downloaden
-- Vergleich mit txt-Daten aus diesem Quartal
--- Alle Reports (10-K/Q) müssen vorhanden sein
--- Die txt-Dateien dürfen keine Zeilen enthalten, die nicht in den XML Daten vorhanden sind
--- Prüfen ob

- zippen von xml dateien


Done
====
14.05.2021 - diverse optimierungen in pre-parsing
           - neu download mit prüfung auf grösse, da irgendwie die falschen Daten vorhanden sind...
           - donwload-xml Namen mit ADSH, damit eindeutigkeit sicher gestellt ist
13.05.2021 - diverse pre-parsing korrekturen
11.05.2021 - organistaion directory
               -https://docs.python-guide.org/writing/structure/
09.05.2021  - Masstest für teiltestset mit erneutem parsing
            - nur relevante statements parsen
08.05.2021  - line Eintrag aufgrund Hierarchie berechnet
04.05.2021  - alle Daten nochmals durchparsen
03.05.2021  - Fehler prüfen -> "huge text node"
02.05.2021  - Quartal durchparsen -> nur 2 'leere' numfiles können nicht vearbeitet werden
            - Automatisierung des Downloads und aufbereiten der Daten
            -- Die Daten werden laufend nachgeladen und aufbereitet
            -- Pro Report werden die Daten in eigene Dateien in Jahr/Monats/Tag (processingday)
30.04.2021  - pre und num files erzeugen..
            -- erste Version läuft, nächster Schritt komplettes quartal durchparsen
            - IFRS Standard Beispiel finden und prüfen
26.04.2021  - aufteilen code für index und xml files
22.04.2021  - Verwaltung Index Files
             -- bereits abgeschlossene nicht nochmals neuladen -> Status in Progress / finished
             -- finished wird erreicht wenn neues File vorhanden ist
09.04.2021  - Automatisierung des Downloads und aufbereiten der Daten
            -- in die DB soll auch der Name des SecFeedFiles aufgenommen werden, aus welchem die Daten stammen
            -- vor dem Inserten der Daten müssen die Einträge entfernt werden, die bereits vorhanden sind
            --- Spalte mit SecFeedFilenamen füllen
            --- Daten mit Schlüssel SecFeedFile laden und aufgrund adsh bereinigen
09.04.2021  - Sicherstellen, dass das Ergänzen der Informationen funktioniert.
              kleinere Pakete sichern, nicht alles auf einmal
09.04.2021  - Duplicate Check hinzufügen -> sicherstellen, dass jede ADSH nur einmal vorhanden ist.
              Offenbar gibt es mit den März Daten Probleme, entweder doppelt in der Datei, oder aber bereits im Februar vorhanden
05.04.2021  Flywayconfig  und installation