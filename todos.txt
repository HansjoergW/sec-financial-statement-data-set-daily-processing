Next
====

Reparse Tools -> get_xml_fils_from_sec muss irgendwie aufgeteilt werden
 - > finden eines bestimmten Testset aus aus der DB sollte nicht Bestandteil von
     Rparse Tool sein, dass sollte nur liste mit adshs bekommen

     -> TestsetcreatorTool ->


- da das tag herauslesen jetzt einigermassen funktioniert, könnte man vlt einen besseren vergleich
  zwischen inhalt in zip und geparsten files machen.
  -> prüfen, welche genau übereinstimmen,
  -> prüfen, welche Inhalte zuviel gescannt werden
  -> z.B Report 0000014272-21-000066 -> hier werden beim parsen viel mehr reports gefunden, als in zip

  es wäre auch gut für die zip datei die selbe stmt-taglist struktur aufzubauen, das würde den vergleich einfacher machen,
  so dass man auch sieht, was für statments es in zip gibt und wie gross die anzahl tags darin ist

  ziel müsste sein, dass über Tabellen machen zu können.
  - Aufbereiten der pre.txt in eine Tabelle mit den adsh, stmt, report, taglist, taglist-länge
  - Aufbereiten der geparsten Daten in ähnlicher Form, zusätzlich für die gefunden Reports die role und root-node,
    sowie der Key, aufgrund dessen der Eintrag klassifziert wurde



- Pre Spezialfälle
    - "nummerierte Tags", die dann keine to from beziehung mehr ermöglichen:
       0000018255-21-000004, 0000016160-21-000018

- Aufbau erweiterbarer "PRE" Test, in welchem mann spezialfälle direkt in ein testdaten verzeichnis ablegen
  kann. der Test iteriert dann über alle Testfiles und prüft die Ergebnisse.
  Die Prüfungen müsste man vermutlich auch irgendwie standardisieren, bzw. die erwartete CSV auch Datei hinstellen.


- berechnen pre-xml
 - vlt sollte man sich zuerst nur auf fehlende reports konzentrieren
   -> könnte man höchstens über die anzahl lösen ->
   -> das grösste problem ist aber, dass die report nummer nicht stimmt
   -> vlt wäre es besser, zuerst nur stmt:anzahl Zeilen zu vergleichen, also wirklich eine spezielles report tag  zu generieren
 - nicht beachten der speziellen "srt" Zeilen


- quartal vergleichen
-- pre zip
    # problem, die gruppierung muss beim vergleich beachtet werden.
    # es wird so eine art universelle gruppe benötigt.. so was wie
    # statement-anz rows..
    # vlt. würde es auch helfen, wenn nur die relevanten statements gefiltert
    # werden. Vlt würde dann die Nummerierung stimmen..


- status index file über https://www.sec.gov/Archives/edgar/monthly/index.json auswerten
  -> File SecIndexIndexJsonParsing.py
  -> man sieht die letzte Bearbeitung
  -> man sieht die letzte Datei


- bessere und klarere Log Statements.. irgendwie standardisieren. Vlt auch so etwas wie eine processstatus am Ende

Later
=====

Ziel: laufendes herunterladen der neuen Reports als XML und bereitstellen der Daten als CSV Datei

- Massentest über ganzes Quartal
-- XML-Dateien für komplettes Quartal downloaden
-- Vergleich mit txt-Daten aus diesem Quartal
--- Alle Reports (10-K/Q) müssen vorhanden sein
--- Die txt-Dateien dürfen keine Zeilen enthalten, die nicht in den XML Daten vorhanden sind
--- Prüfen ob

- zippen von xml dateien

- Reihenfolge der Spalten in den geparsten CSVs in der selben Reihenfolge wie original, damit es änlicher iwrd


Done
====
14.05.2021 - diverse optimierungen in pre-parsing
           - neu download mit prüfung auf grösse, da irgendwie die falschen Daten vorhanden sind...
           - donwload-xml Namen mit ADSH, damit eindeutigkeit sicher gestellt ist
13.05.2021 - diverse pre-parsing korrekturen
11.05.2021 - organistaion directory
               -https://docs.python-guide.org/writing/structure/
09.05.2021  - Masstest für teiltestset mit erneutem parsing
            - nur relevante statements parsen
08.05.2021  - line Eintrag aufgrund Hierarchie berechnet
04.05.2021  - alle Daten nochmals durchparsen
03.05.2021  - Fehler prüfen -> "huge text node"
02.05.2021  - Quartal durchparsen -> nur 2 'leere' numfiles können nicht vearbeitet werden
            - Automatisierung des Downloads und aufbereiten der Daten
            -- Die Daten werden laufend nachgeladen und aufbereitet
            -- Pro Report werden die Daten in eigene Dateien in Jahr/Monats/Tag (processingday)
30.04.2021  - pre und num files erzeugen..
            -- erste Version läuft, nächster Schritt komplettes quartal durchparsen
            - IFRS Standard Beispiel finden und prüfen
26.04.2021  - aufteilen code für index und xml files
22.04.2021  - Verwaltung Index Files
             -- bereits abgeschlossene nicht nochmals neuladen -> Status in Progress / finished
             -- finished wird erreicht wenn neues File vorhanden ist
09.04.2021  - Automatisierung des Downloads und aufbereiten der Daten
            -- in die DB soll auch der Name des SecFeedFiles aufgenommen werden, aus welchem die Daten stammen
            -- vor dem Inserten der Daten müssen die Einträge entfernt werden, die bereits vorhanden sind
            --- Spalte mit SecFeedFilenamen füllen
            --- Daten mit Schlüssel SecFeedFile laden und aufgrund adsh bereinigen
09.04.2021  - Sicherstellen, dass das Ergänzen der Informationen funktioniert.
              kleinere Pakete sichern, nicht alles auf einmal
09.04.2021  - Duplicate Check hinzufügen -> sicherstellen, dass jede ADSH nur einmal vorhanden ist.
              Offenbar gibt es mit den März Daten Probleme, entweder doppelt in der Datei, oder aber bereits im Februar vorhanden
05.04.2021  Flywayconfig  und installation